{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TIR-Enabled Ensemble with Self-Consistency\n# Based on AIMO Progress Prize 1 & 2 winning approaches\n\nimport os\nimport sys\n\n# Suppress common warnings and CUDA messages\nos.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['CUDA_LAUNCH_BLOCKING'] = '0'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid tokenizer warnings in parallel\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Suppress TensorFlow/CUDA initialization warnings\nimport logging\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\nlogging.getLogger('absl').setLevel(logging.ERROR)\n\n# Fix for protobuf MessageFactory.GetPrototype compatibility issue\nimport google.protobuf.message_factory as message_factory\nfrom google.protobuf import descriptor_pool\n\ndef GetPrototype(self, descriptor):\n    return self.GetMessageClass(descriptor)\n\nif not hasattr(message_factory.MessageFactory, 'GetPrototype'):\n    message_factory.MessageFactory.GetPrototype = GetPrototype\n\nimport pandas as pd\nimport polars as pl\nimport re\nimport subprocess\nimport tempfile\nfrom typing import Optional, List, Tuple\nfrom collections import Counter\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport kaggle_evaluation.aimo_3_inference_server"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport tempfile\nimport os\n\ndef execute_python_code(code: str, timeout: int = 10) -> tuple[Optional[int], str]:\n    \"\"\"Safely execute Python code and return the result.\"\"\"\n    try:\n        # Add common math libraries automatically\n        enhanced_code = \"\"\"import math\nimport numpy as np\ntry:\n    import sympy as sp\n    from sympy import *\nexcept ImportError:\n    pass\n\n\"\"\" + code\n        \n        # Ensure last line is printed if it's not already\n        lines = enhanced_code.strip().split('\\n')\n        if lines and 'print(' not in lines[-1] and 'import' not in lines[-1]:\n            # Remove trailing comments\n            if '#' in lines[-1]:\n                lines[-1] = lines[-1].split('#')[0]\n            lines[-1] = 'print(' + lines[-1].strip() + ')'\n        enhanced_code = '\\n'.join(lines)\n        \n        # Create a safe execution environment\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n            f.write(enhanced_code)\n            temp_file = f.name\n        \n        try:\n            result = subprocess.run(\n                ['python3', temp_file],\n                capture_output=True,\n                text=True,\n                timeout=timeout\n            )\n            \n            output = result.stdout + result.stderr\n            \n            # Try to extract the last number printed\n            lines = output.strip().split('\\n')\n            for line in reversed(lines):\n                # Look for numbers in the output\n                numbers = re.findall(r'\\b(\\d{1,5})\\b', line)\n                if numbers:\n                    try:\n                        answer = int(numbers[-1])\n                        if 0 <= answer <= 99999:\n                            return answer, output\n                    except:\n                        pass\n            \n            return None, output\n        finally:\n            os.unlink(temp_file)\n    except Exception as e:\n        return None, str(e)\n\ndef extract_code_from_response(text: str) -> Optional[str]:\n    \"\"\"Extract Python code blocks from model response.\"\"\"\n    # Look for code blocks\n    code_patterns = [\n        r'```python\\n(.*?)```',\n        r'```\\n(.*?)```',\n    ]\n    \n    for pattern in code_patterns:\n        matches = re.findall(pattern, text, re.DOTALL)\n        if matches:\n            return matches[-1].strip()\n    \n    return None\n\ndef extract_answer(text: str) -> Optional[int]:\n    \"\"\"Extract numerical answer from model output (enhanced patterns).\"\"\"\n    patterns = [\n        r'\\\\boxed\\{(\\d{1,5})\\}',\n        r'oxed\\{(\\d{1,5})\\}',  # Sometimes \\\\ is dropped\n        r'#### (\\d{1,5})',\n        r'(?i)final\\s+answer\\s*(?:is|:)?\\s*(\\d{1,5})',\n        r'(?:answer|result|solution)(?:\\s+is)?:?\\s*(\\d{1,5})',\n        r'=\\s*(\\d{1,5})(?:\\s|$|\\.|,)',\n        r'(\\d{1,5})(?:\\s+(?:is the|as the) answer)',\n        r'therefore.*?(\\d{1,5})',\n        r'answer\\s*=\\s*(\\d{1,5})',  # Variable assignment\n    ]\n    \n    for pattern in patterns:\n        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n        if matches:\n            try:\n                answer = int(matches[-1])\n                if 0 <= answer <= 99999:\n                    return answer\n            except ValueError:\n                continue\n    \n    # Try to find any 1-5 digit number near end of text\n    last_500 = text[-500:] if len(text) > 500 else text\n    numbers = re.findall(r'\\b(\\d{1,5})\\b', last_500)\n    if numbers:\n        try:\n            answer = int(numbers[-1])\n            if 0 <= answer <= 99999:\n                return answer\n        except:\n            pass\n    \n    return None\n\ndef validate_answer(answer: int) -> bool:\n    return isinstance(answer, int) and 0 <= answer <= 99999"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed MathTools and symbolic solver to comply with competition rules\n",
    "# Competition requires model-based solutions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport glob\nimport threading\n\ndef find_model_path(base_path):\n    \"\"\"Auto-detect the actual model directory within a dataset.\"\"\"\n    if not os.path.exists(base_path):\n        return None\n    \n    if os.path.exists(os.path.join(base_path, 'config.json')):\n        return base_path\n    \n    for root, dirs, files in os.walk(base_path):\n        if 'config.json' in files:\n            return root\n    \n    return None\n\n\nclass ModelConfig:\n    \"\"\"Configuration for each model strategy.\"\"\"\n    \n    def __init__(self, name: str, kaggle_path: str, hf_path: str, temp: float = 0.7, \n                 top_p: float = 0.9, top_k: int = 50, needs_trust: bool = False, \n                 num_samples: int = 1):\n        self.name = name\n        self.kaggle_path = kaggle_path\n        self.hf_path = hf_path\n        self.temp = temp\n        self.top_p = top_p\n        self.top_k = top_k\n        self.needs_trust = needs_trust\n        self.num_samples = num_samples\n    \n    def get_path(self):\n        if self.kaggle_path:\n            actual_path = find_model_path(self.kaggle_path)\n            if actual_path:\n                return actual_path\n        return self.hf_path\n\n\nclass EnsembleSolver:\n    \"\"\"TIR-enabled solver with parallel sampling and early stopping.\"\"\"\n    \n    def __init__(self, early_stop_threshold: int = 10):\n        self.early_stop_threshold = early_stop_threshold  # Increased from 5 to 10\n        \n        # Increase samples per model for more diversity\n        self.model_configs = [\n            # Model 1: Qwen2.5-Math-1.5B (fast, MORE samples)\n            ModelConfig(\n                \"Qwen-1.5B\",\n                '/kaggle/input/qwen2-5-math-1-5b-instruct',\n                'Qwen/Qwen2.5-Math-1.5B-Instruct',\n                temp=0.7, top_p=0.9, top_k=50, num_samples=5  # Increased from 3\n            ),\n            \n            # Model 2: Qwen2.5-Math-7B - Higher temps for more exploration\n            ModelConfig(\n                \"Qwen-7B-Low\",\n                '/kaggle/input/qwen2-5-math-7b-instruct',\n                'Qwen/Qwen2.5-Math-7B-Instruct',\n                temp=0.6, top_p=0.9, top_k=40, num_samples=3  # Increased from 2\n            ),\n            ModelConfig(\n                \"Qwen-7B-Mid\",\n                '/kaggle/input/qwen2-5-math-7b-instruct',\n                'Qwen/Qwen2.5-Math-7B-Instruct',\n                temp=0.8, top_p=0.92, top_k=50, num_samples=3  # Increased temp & samples\n            ),\n            ModelConfig(\n                \"Qwen-7B-High\",\n                '/kaggle/input/qwen2-5-math-7b-instruct',\n                'Qwen/Qwen2.5-Math-7B-Instruct',\n                temp=1.0, top_p=0.95, top_k=60, num_samples=4  # Much higher temp for creativity\n            ),\n            \n            # Model 3: DeepSeek-Math-7B-RL - More samples\n            ModelConfig(\n                \"DeepSeek-Low\",\n                '/kaggle/input/deepseek-math-7b-rl',\n                'deepseek-ai/deepseek-math-7b-rl',\n                temp=0.5, top_p=0.88, top_k=35, num_samples=3,  # Increased\n                needs_trust=True\n            ),\n            ModelConfig(\n                \"DeepSeek-Mid\",\n                '/kaggle/input/deepseek-math-7b-rl',\n                'deepseek-ai/deepseek-math-7b-rl',\n                temp=0.7, top_p=0.92, top_k=50, num_samples=3,  # Increased\n                needs_trust=True\n            ),\n            ModelConfig(\n                \"DeepSeek-High\",\n                '/kaggle/input/deepseek-math-7b-rl',\n                'deepseek-ai/deepseek-math-7b-rl',\n                temp=0.9, top_p=0.95, top_k=60, num_samples=4,  # Much higher for exploration\n                needs_trust=True\n            ),\n            \n            # Model 4: MAmmoTH-7B - More samples\n            ModelConfig(\n                \"MAmmoTH-7B\",\n                '/kaggle/input/mammoth-7b-mistral',\n                'TIGER-Lab/MAmmoTH-7B-Mistral',\n                temp=0.8, top_p=0.92, top_k=50, num_samples=5,  # Increased from 3\n                needs_trust=True\n            ),\n        ]\n    \n    def solve_with_model_single(self, problem: str, model, tokenizer, model_config: ModelConfig, \n                                sample_idx: int, stop_event: threading.Event) -> Optional[int]:\n        \"\"\"Generate single sample from a loaded model.\"\"\"\n        try:\n            import torch\n            \n            # Check if we should stop early\n            if stop_event.is_set():\n                return None\n            \n            # Improved prompt with more explicit code execution guidance\n            prompt = f\"\"\"You are solving a challenging International Mathematical Olympiad problem. \n\nProblem:\n{problem}\n\nApproach:\n1. Read the problem carefully and identify what's being asked\n2. Break down the problem into smaller steps\n3. Use Python code to compute intermediate results - wrap code in ```python blocks\n4. For combinatorics/number theory, write explicit Python calculations\n5. Verify your answer makes sense given the constraints\n6. The answer MUST be an integer between 0 and 99999\n\nImportant:\n- Write Python code to calculate the answer when possible\n- Use sympy for symbolic math, numpy for numerical computations\n- Print intermediate results to verify your work\n- Put your final answer in \\\\boxed{{answer}} format\n\nSolution:\"\"\"\n\n            # Try to use chat template if available, otherwise use plain text\n            if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None:\n                messages = [{\"role\": \"user\", \"content\": prompt}]\n                text = tokenizer.apply_chat_template(\n                    messages,\n                    tokenize=False,\n                    add_generation_prompt=True\n                )\n            else:\n                text = prompt\n\n            inputs = tokenizer([text], return_tensors=\"pt\")\n            if hasattr(model, 'device'):\n                inputs = inputs.to(model.device)\n\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=3072,  # Increased from 2048 for longer solutions\n                    temperature=model_config.temp,\n                    do_sample=True,\n                    top_p=model_config.top_p,\n                    top_k=model_config.top_k,\n                    pad_token_id=tokenizer.eos_token_id,\n                )\n\n            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n            \n            # Try Tool-Integrated Reasoning first (prioritize code)\n            code = extract_code_from_response(response)\n            answer = None\n            \n            if code:\n                code_answer, _ = execute_python_code(code, timeout=20)  # Increased timeout\n                if code_answer is not None:\n                    answer = code_answer\n            \n            # Fallback to text extraction\n            if answer is None:\n                answer = extract_answer(response)\n            \n            if answer is not None and validate_answer(answer):\n                return answer\n            \n            return None\n            \n        except Exception as e:\n            return None\n    \n    def solve_with_model(self, problem: str, model_config: ModelConfig, \n                        stop_event: threading.Event) -> List[int]:\n        \"\"\"Solve using one model, generate multiple samples with early stopping.\"\"\"\n        try:\n            import torch\n            from transformers import AutoTokenizer, AutoModelForCausalLM\n            import gc\n            \n            # Check early stop before loading model\n            if stop_event.is_set():\n                print(f\"    [{model_config.name}] Skipped (early stop)\")\n                return []\n            \n            model_path = model_config.get_path()\n            \n            if not model_path:\n                print(f\"    [{model_config.name}] Not found\")\n                return []\n            \n            print(f\"  [{model_config.name}] Loading ({model_config.num_samples} samples)...\")\n            \n            try:\n                tokenizer = AutoTokenizer.from_pretrained(\n                    model_path, \n                    use_fast=True,\n                    trust_remote_code=model_config.needs_trust\n                )\n                \n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    device_map=\"auto\" if torch.cuda.is_available() else None,\n                    trust_remote_code=model_config.needs_trust,\n                    low_cpu_mem_usage=True\n                )\n            except Exception as e:\n                print(f\"    Load failed: {str(e)[:80]}\")\n                return []\n            \n            if torch.cuda.is_available():\n                model.eval()\n            \n            answers = []\n            \n            # Parallel sampling for this model\n            with ThreadPoolExecutor(max_workers=model_config.num_samples) as executor:\n                futures = {\n                    executor.submit(self.solve_with_model_single, problem, model, \n                                  tokenizer, model_config, i, stop_event): i\n                    for i in range(model_config.num_samples)\n                }\n                \n                for future in as_completed(futures):\n                    if stop_event.is_set():\n                        break\n                    try:\n                        answer = future.result()\n                        if answer is not None:\n                            answers.append(answer)\n                    except Exception:\n                        pass\n            \n            # Cleanup\n            del model\n            del tokenizer\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            gc.collect()\n            \n            if answers:\n                print(f\"    ‚Üí Got {len(answers)}/{model_config.num_samples} answers: {answers}\")\n            else:\n                print(f\"    ‚Üí No valid answers\")\n            \n            return answers\n            \n        except Exception as e:\n            print(f\"  Error: {str(e)[:80]}\")\n            return []\n    \n    def smart_voting(self, all_answers: List[int]) -> Optional[int]:\n        \"\"\"\n        Improved voting: require stronger consensus before accepting an answer\n        \"\"\"\n        if not all_answers:\n            return None\n        \n        vote_counts = Counter(all_answers)\n        total_votes = len(all_answers)\n        \n        # Get most common answer\n        best_answer, best_count = vote_counts.most_common(1)[0]\n        \n        # Require at least 30% consensus (increased from 40% since we have more samples)\n        if best_count >= max(3, total_votes * 0.3):\n            # Additional filtering: reject obvious bad patterns\n            # If best answer is 0 or 1 and it's a complex problem, be suspicious\n            if best_answer in [0, 1] and best_count < total_votes * 0.5:\n                # Check if there's a better non-trivial answer\n                for ans, count in vote_counts.most_common(5):\n                    if ans not in [0, 1] and count >= max(2, total_votes * 0.2):\n                        return ans\n            \n            # Reject 99999 unless strong consensus\n            if best_answer == 99999 and best_count < total_votes * 0.4:\n                for ans, count in vote_counts.most_common(5):\n                    if ans != 99999 and count >= max(2, total_votes * 0.2):\n                        return ans\n            \n            return best_answer\n        \n        # If no strong consensus, take most common that's not 0/1\n        for ans, count in vote_counts.most_common(10):\n            if ans not in [0, 1]:\n                return ans\n        \n        # Last resort: just return most common\n        return best_answer\n    \n    def solve_with_ensemble(self, problem: str) -> Optional[int]:\n        \"\"\"Run all models with parallel sampling and early stopping.\"\"\"\n        all_answers = []\n        stop_event = threading.Event()\n        \n        print(f\"  Running TIR ensemble with early stopping (threshold={self.early_stop_threshold})...\")\n        \n        for model_config in self.model_configs:\n            # Check for early stop after each model\n            if stop_event.is_set():\n                print(f\"  ‚èπÔ∏è  Early stop triggered, skipping remaining models\")\n                break\n            \n            answers = self.solve_with_model(problem, model_config, stop_event)\n            all_answers.extend(answers)\n            \n            # Check if we have consensus for early stopping\n            if len(all_answers) >= self.early_stop_threshold:\n                counts = Counter(all_answers)\n                most_common_ans, count = counts.most_common(1)[0]\n                if count >= self.early_stop_threshold:\n                    print(f\"  üéØ Early stop! Answer {most_common_ans} appeared {count} times\")\n                    stop_event.set()\n                    break\n        \n        if not all_answers:\n            return None\n        \n        # Smart voting\n        best_answer = self.smart_voting(all_answers)\n        vote_counts = Counter(all_answers)\n        \n        print(f\"  Total samples: {len(all_answers)}\")\n        print(f\"  Vote distribution: {dict(vote_counts.most_common(5))}\")\n        print(f\"  Smart voting result: {best_answer}\")\n        \n        return best_answer\n    \n    def solve_problem(self, problem_id: str, problem_text: str) -> int:\n        answer = self.solve_with_ensemble(problem_text)\n        \n        if answer is None:\n            answer = 0\n        \n        if not validate_answer(answer):\n            answer = abs(answer) % 100000\n        \n        return answer\n\nsolver = EnsembleSolver(early_stop_threshold=10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"TIR-ENABLED ENSEMBLE with Smart Voting & Early Stopping\")\nprint(\"=\" * 70)\nprint(\"Based on AIMO Prize 1 & 2 winning solutions + GPT-OSS-120B notebook:\")\nprint(\"  ‚úì Tool-Integrated Reasoning (Python code execution)\")\nprint(\"  ‚úì Enhanced code execution (auto math library imports)\")\nprint(\"  ‚úì Parallel sampling per model (ThreadPoolExecutor)\")\nprint(\"  ‚úì Early stopping when 5 samples agree (saves time!)\")\nprint(\"  ‚úì Smart voting (filters outliers, requires consensus)\")\nprint(\"  ‚úì Improved answer extraction (10+ patterns)\")\nprint(\"  ‚úì ~20 total samples per problem (or less with early stop)\")\nprint(\"\")\nprint(\"Models (4 different models, 8 configs):\")\nprint(\"  ‚Ä¢ Qwen2.5-Math-1.5B (3 parallel samples)\")\nprint(\"  ‚Ä¢ Qwen2.5-Math-7B (6 parallel samples: 3 temps √ó 2 each)\")\nprint(\"  ‚Ä¢ DeepSeek-Math-7B-RL (6 parallel samples: 3 temps √ó 2 each)\")\nprint(\"  ‚Ä¢ MAmmoTH-7B-Mistral (3 parallel samples)\")\nprint(\"=\" * 70)\n\n# Global tracking for accuracy calculation (local mode only)\npredictions = {}\ncorrect_count = 0\ntotal_count = 0\nground_truth = {}\n\n# Load reference data for local testing\nreference_path = '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv'\nif os.path.exists(reference_path):\n    df = pd.read_csv(reference_path)\n    # Store ground truth if available\n    if \"answer\" in df.columns:\n        ground_truth = dict(zip(df[\"id\"], df[\"answer\"]))\n        print(f\"\\nüìã Loaded {len(ground_truth)} reference answers for accuracy tracking\")\n\ndef predict(id_series: pl.Series, problem_series: pl.Series, *extra) -> pl.DataFrame:\n    \"\"\"Prediction function for inference server.\n\n    The gateway unpacks the row DataFrame by columns, so we receive Series objects.\n    For reference.csv (3 cols): id, problem, answer\n    For test.csv (2 cols): id, problem\n    \n    Args:\n        id_series: Series with the problem ID\n        problem_series: Series with the problem text  \n        *extra: Any additional columns (like 'answer' in reference.csv)\n\n    Returns:\n        DataFrame with 'id' and 'answer' columns\n    \"\"\"\n    global correct_count, total_count, predictions\n\n    try:\n        # Extract values from Series\n        question_id = id_series[0]\n        question_text = problem_series[0]\n        \n        print(\"=\" * 60)\n        print(f\"ID: {question_id}\")\n        print(f\"Question: {question_text[:200]}...\")\n        \n        answer = solver.solve_problem(question_id, question_text)\n        \n        # Store prediction\n        predictions[question_id] = answer\n        \n        # Check accuracy if ground truth available\n        total_count += 1\n        if question_id in ground_truth:\n            gt = ground_truth[question_id]\n            is_correct = (answer == gt)\n            if is_correct:\n                correct_count += 1\n            status = \"‚úÖ\" if is_correct else \"‚ùå\"\n            print(f\"\\nAnswer: {answer} | Ground Truth: {gt} | {status}\")\n            print(f\"üìä Running Accuracy: {correct_count}/{total_count} ({100*correct_count/total_count:.1f}%)\")\n        else:\n            print(f\"\\nAnswer: {answer}\")\n        \n        print(\"=\" * 60 + \"\\n\")\n        \n        return pl.DataFrame({\"id\": [question_id], \"answer\": [answer]})\n    except Exception as e:\n        print(f\"Prediction error: {e}\")\n        import traceback\n        traceback.print_exc()\n        \n        # Try to extract ID\n        try:\n            fallback_id = id_series[0]\n        except:\n            fallback_id = \"unknown\"\n            \n        return pl.DataFrame({\"id\": [fallback_id], \"answer\": [0]})\n\ninference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n\n# Auto-detect: competition rerun vs local testing\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print(\"\\n[COMPETITION MODE] Starting inference server...\")\n    inference_server.serve()\nelse:\n    print(\"\\n[LOCAL MODE] Running local gateway on reference.csv...\")\n    inference_server.run_local_gateway((reference_path,))\n    \n    # Print final accuracy summary\n    if ground_truth and total_count > 0:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"üìä FINAL ACCURACY SUMMARY\")\n        print(\"=\" * 60)\n        print(f\"Correct: {correct_count}/{total_count}\")\n        print(f\"Accuracy: {100*correct_count/total_count:.1f}%\")\n        print(\"=\" * 60)\n        \n        # Show details of incorrect predictions\n        incorrect = []\n        for qid, pred in predictions.items():\n            if qid in ground_truth:\n                gt = ground_truth[qid]\n                if pred != gt:\n                    incorrect.append((qid, pred, gt))\n        \n        if incorrect:\n            print(\"\\n‚ùå Incorrect Predictions:\")\n            for qid, pred, gt in incorrect:\n                print(f\"  {qid}: predicted={pred}, actual={gt}\")\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}