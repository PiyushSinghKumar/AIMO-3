{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TIR-Enabled Ensemble with Self-Consistency\n# Based on AIMO Progress Prize 1 & 2 winning approaches\n\n# CONTROL: Set to True to test on reference.csv, False to submit\nTEST_MODE = True  # Set to False before submission!\n\nimport os\nimport sys\n\nos.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport polars as pl\nimport re\nimport subprocess\nimport tempfile\nfrom typing import Optional, List, Tuple\nfrom collections import Counter\nimport kaggle_evaluation.aimo_3_inference_server"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport tempfile\nimport os\n\ndef execute_python_code(code: str, timeout: int = 10) -> tuple[Optional[int], str]:\n    \"\"\"Safely execute Python code and return the result.\"\"\"\n    try:\n        # Create a safe execution environment\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n            f.write(code)\n            temp_file = f.name\n        \n        try:\n            result = subprocess.run(\n                ['python3', temp_file],\n                capture_output=True,\n                text=True,\n                timeout=timeout\n            )\n            \n            output = result.stdout + result.stderr\n            \n            # Try to extract the last number printed\n            lines = output.strip().split('\\n')\n            for line in reversed(lines):\n                # Look for numbers in the output\n                numbers = re.findall(r'\\b(\\d{1,5})\\b', line)\n                if numbers:\n                    try:\n                        answer = int(numbers[-1])\n                        if 0 <= answer <= 99999:\n                            return answer, output\n                    except:\n                        pass\n            \n            return None, output\n        finally:\n            os.unlink(temp_file)\n    except Exception as e:\n        return None, str(e)\n\ndef extract_code_from_response(text: str) -> Optional[str]:\n    \"\"\"Extract Python code blocks from model response.\"\"\"\n    # Look for code blocks\n    code_patterns = [\n        r'```python\\n(.*?)```',\n        r'```\\n(.*?)```',\n    ]\n    \n    for pattern in code_patterns:\n        matches = re.findall(pattern, text, re.DOTALL)\n        if matches:\n            return matches[-1].strip()\n    \n    return None\n\ndef extract_answer(text: str) -> Optional[int]:\n    \"\"\"Extract numerical answer from model output.\"\"\"\n    patterns = [\n        r'\\\\boxed\\{(\\d{1,5})\\}',\n        r'#### (\\d{1,5})',\n        r'(?:final answer|answer|result|solution)(?:\\s+is)?:?\\s*(\\d{1,5})',\n        r'=\\s*(\\d{1,5})(?:\\s|$|\\.|,)',\n        r'(\\d{1,5})(?:\\s+(?:is the|as the) answer)',\n        r'therefore.*?(\\d{1,5})',\n    ]\n    \n    for pattern in patterns:\n        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n        if matches:\n            try:\n                answer = int(matches[-1])\n                if 0 <= answer <= 99999:\n                    return answer\n            except ValueError:\n                continue\n    \n    # Try to find any 1-5 digit number near end of text\n    last_500 = text[-500:] if len(text) > 500 else text\n    numbers = re.findall(r'\\b(\\d{1,5})\\b', last_500)\n    if numbers:\n        try:\n            answer = int(numbers[-1])\n            if 0 <= answer <= 99999:\n                return answer\n        except:\n            pass\n    \n    return None\n\ndef validate_answer(answer: int) -> bool:\n    return isinstance(answer, int) and 0 <= answer <= 99999"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed MathTools and symbolic solver to comply with competition rules\n",
    "# Competition requires model-based solutions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport glob\n\ndef find_model_path(base_path):\n    \"\"\"Auto-detect the actual model directory within a dataset.\"\"\"\n    if not os.path.exists(base_path):\n        return None\n    \n    if os.path.exists(os.path.join(base_path, 'config.json')):\n        return base_path\n    \n    for root, dirs, files in os.walk(base_path):\n        if 'config.json' in files:\n            return root\n    \n    return None\n\n\nclass ModelConfig:\n    \"\"\"Configuration for each model strategy.\"\"\"\n    \n    def __init__(self, name: str, kaggle_path: str, hf_path: str, temp: float = 0.7, \n                 top_p: float = 0.9, top_k: int = 50, needs_trust: bool = False, \n                 num_samples: int = 1):\n        self.name = name\n        self.kaggle_path = kaggle_path\n        self.hf_path = hf_path\n        self.temp = temp\n        self.top_p = top_p\n        self.top_k = top_k\n        self.needs_trust = needs_trust\n        self.num_samples = num_samples\n    \n    def get_path(self):\n        if self.kaggle_path:\n            actual_path = find_model_path(self.kaggle_path)\n            if actual_path:\n                return actual_path\n        return self.hf_path\n\n\nclass EnsembleSolver:\n    \"\"\"TIR-enabled solver with multiple sampling and smart voting.\"\"\"\n    \n    def __init__(self):\n        # ALL 4 models with multiple samples = ~20-24 total samples\n        self.model_configs = [\n            # Model 1: Qwen2.5-Math-1.5B (fast, 3 samples)\n            ModelConfig(\n                \"Qwen-1.5B\",\n                '/kaggle/input/qwen2-5-math-1-5b-instruct',\n                'Qwen/Qwen2.5-Math-1.5B-Instruct',\n                temp=0.5, top_p=0.9, top_k=40, num_samples=3\n            ),\n            \n            # Model 2: Qwen2.5-Math-7B (3 configs, 2 samples each = 6 samples)\n            ModelConfig(\n                \"Qwen-7B-Low\",\n                '/kaggle/input/qwen2-5-math-7b-instruct',\n                'Qwen/Qwen2.5-Math-7B-Instruct',\n                temp=0.4, top_p=0.88, top_k=35, num_samples=2\n            ),\n            ModelConfig(\n                \"Qwen-7B-Mid\",\n                '/kaggle/input/qwen2-5-math-7b-instruct',\n                'Qwen/Qwen2.5-Math-7B-Instruct',\n                temp=0.6, top_p=0.9, top_k=45, num_samples=2\n            ),\n            ModelConfig(\n                \"Qwen-7B-High\",\n                '/kaggle/input/qwen2-5-math-7b-instruct',\n                'Qwen/Qwen2.5-Math-7B-Instruct',\n                temp=0.8, top_p=0.95, top_k=60, num_samples=2\n            ),\n            \n            # Model 3: DeepSeek-Math-7B-RL (3 configs, 2 samples each = 6 samples)\n            ModelConfig(\n                \"DeepSeek-Low\",\n                '/kaggle/input/deepseek-math-7b-rl',\n                'deepseek-ai/deepseek-math-7b-rl',\n                temp=0.3, top_p=0.85, top_k=30, num_samples=2,\n                needs_trust=True\n            ),\n            ModelConfig(\n                \"DeepSeek-Mid\",\n                '/kaggle/input/deepseek-math-7b-rl',\n                'deepseek-ai/deepseek-math-7b-rl',\n                temp=0.6, top_p=0.9, top_k=45, num_samples=2,\n                needs_trust=True\n            ),\n            ModelConfig(\n                \"DeepSeek-High\",\n                '/kaggle/input/deepseek-math-7b-rl',\n                'deepseek-ai/deepseek-math-7b-rl',\n                temp=0.8, top_p=0.95, top_k=55, num_samples=2,\n                needs_trust=True\n            ),\n            \n            # Model 4: MAmmoTH-7B (3 samples)\n            ModelConfig(\n                \"MAmmoTH-7B\",\n                '/kaggle/input/mammoth-7b-mistral',\n                'TIGER-Lab/MAmmoTH-7B-Mistral',\n                temp=0.6, top_p=0.9, top_k=45, num_samples=3,\n                needs_trust=True\n            ),\n        ]\n    \n    def solve_with_model(self, problem: str, model_config: ModelConfig) -> List[int]:\n        \"\"\"Solve using one model, generate multiple samples.\"\"\"\n        try:\n            import torch\n            from transformers import AutoTokenizer, AutoModelForCausalLM\n            import gc\n            \n            model_path = model_config.get_path()\n            \n            if not model_path:\n                print(f\"    [{model_config.name}] Not found\")\n                return []\n            \n            print(f\"  [{model_config.name}] Loading ({model_config.num_samples} samples)...\")\n            \n            try:\n                tokenizer = AutoTokenizer.from_pretrained(\n                    model_path, \n                    use_fast=True,\n                    trust_remote_code=model_config.needs_trust\n                )\n                \n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                    device_map=\"auto\" if torch.cuda.is_available() else None,\n                    trust_remote_code=model_config.needs_trust,\n                    low_cpu_mem_usage=True\n                )\n            except Exception as e:\n                print(f\"    Load failed: {str(e)[:80]}\")\n                return []\n            \n            if torch.cuda.is_available():\n                model.eval()\n            \n            answers = []\n            \n            # Generate multiple samples\n            for sample_idx in range(model_config.num_samples):\n                try:\n                    # Tool-Integrated Reasoning prompt (like AIMO winners)\n                    prompt = f\"\"\"Solve this International Mathematical Olympiad problem step by step.\n\nProblem:\n{problem}\n\nInstructions:\n- Think through the problem carefully\n- You can write Python code in ```python blocks to help solve it\n- Show your reasoning and calculations\n- The final answer must be an integer between 0 and 99999\n- Write your final answer as: \\\\boxed{{answer}}\n\nSolution:\"\"\"\n\n                    # Try to use chat template if available, otherwise use plain text\n                    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None:\n                        messages = [{\"role\": \"user\", \"content\": prompt}]\n                        text = tokenizer.apply_chat_template(\n                            messages,\n                            tokenize=False,\n                            add_generation_prompt=True\n                        )\n                    else:\n                        text = prompt\n\n                    inputs = tokenizer([text], return_tensors=\"pt\")\n                    if hasattr(model, 'device'):\n                        inputs = inputs.to(model.device)\n\n                    with torch.no_grad():\n                        outputs = model.generate(\n                            **inputs,\n                            max_new_tokens=2048,  # More tokens for better reasoning\n                            temperature=model_config.temp,\n                            do_sample=True,\n                            top_p=model_config.top_p,\n                            top_k=model_config.top_k,\n                            pad_token_id=tokenizer.eos_token_id,\n                        )\n\n                    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n                    \n                    # Try Tool-Integrated Reasoning first\n                    code = extract_code_from_response(response)\n                    answer = None\n                    \n                    if code:\n                        # Execute the code\n                        code_answer, code_output = execute_python_code(code)\n                        if code_answer is not None:\n                            answer = code_answer\n                    \n                    # Fallback to text extraction\n                    if answer is None:\n                        answer = extract_answer(response)\n                    \n                    if answer is not None and validate_answer(answer):\n                        answers.append(answer)\n                \n                except Exception as e:\n                    pass  # Skip failed samples\n            \n            # Cleanup\n            del model\n            del tokenizer\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            gc.collect()\n            \n            if answers:\n                print(f\"    → Got {len(answers)}/{model_config.num_samples} answers: {answers}\")\n            else:\n                print(f\"    → No valid answers\")\n            \n            return answers\n            \n        except Exception as e:\n            print(f\"  Error: {str(e)[:80]}\")\n            return []\n    \n    def smart_voting(self, all_answers: List[int]) -> Optional[int]:\n        \"\"\"\n        Smarter voting strategy:\n        1. Simple majority for clear consensus\n        2. If no majority, prefer answers from larger models\n        3. Ignore extreme outliers (99999, very large numbers for small expected answers)\n        \"\"\"\n        if not all_answers:\n            return None\n        \n        vote_counts = Counter(all_answers)\n        total_votes = len(all_answers)\n        \n        # Get top 3 candidates\n        top_candidates = vote_counts.most_common(3)\n        best_answer, best_count = top_candidates[0]\n        \n        # Strong consensus (>40% agree) - just use it\n        if best_count >= total_votes * 0.4:\n            return best_answer\n        \n        # Weak consensus - apply heuristics\n        # Remove obvious outliers (99999, 0 if it has weak support)\n        filtered_answers = []\n        for ans in all_answers:\n            # Keep 99999 only if it has strong support\n            if ans == 99999 and vote_counts[99999] < total_votes * 0.3:\n                continue\n            # Keep 0 only if it has some support\n            if ans == 0 and vote_counts[0] < 2:\n                continue\n            filtered_answers.append(ans)\n        \n        if filtered_answers:\n            filtered_counts = Counter(filtered_answers)\n            return filtered_counts.most_common(1)[0][0]\n        \n        # Fallback to simple majority\n        return best_answer\n    \n    def solve_with_ensemble(self, problem: str) -> Optional[int]:\n        \"\"\"Run all models with multiple samples and vote.\"\"\"\n        all_answers = []\n        \n        print(f\"  Running TIR ensemble with self-consistency...\")\n        \n        for model_config in self.model_configs:\n            answers = self.solve_with_model(problem, model_config)\n            all_answers.extend(answers)\n        \n        if not all_answers:\n            return None\n        \n        # Smart voting\n        best_answer = self.smart_voting(all_answers)\n        vote_counts = Counter(all_answers)\n        \n        print(f\"  Total samples: {len(all_answers)}\")\n        print(f\"  Vote distribution: {dict(vote_counts.most_common(5))}\")\n        print(f\"  Smart voting result: {best_answer}\")\n        \n        return best_answer\n    \n    def solve_problem(self, problem_id: str, problem_text: str) -> int:\n        answer = self.solve_with_ensemble(problem_text)\n        \n        if answer is None:\n            answer = 0\n        \n        if not validate_answer(answer):\n            answer = abs(answer) % 100000\n        \n        return answer\n\nsolver = EnsembleSolver()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"TIR-ENABLED ENSEMBLE with Smart Voting\")\nprint(\"=\" * 70)\nprint(\"Based on AIMO Prize 1 & 2 winning solutions:\")\nprint(\"  ✓ Tool-Integrated Reasoning (Python code execution)\")\nprint(\"  ✓ Multiple samples per model (self-consistency)\")\nprint(\"  ✓ Smart voting (filters outliers, requires consensus)\")\nprint(\"  ✓ ~20 total samples per problem\")\nprint(\"\")\nprint(\"Models (4 different models, 8 configs):\")\nprint(\"  • Qwen2.5-Math-1.5B (3 samples)\")\nprint(\"  • Qwen2.5-Math-7B (6 samples: 3 temps × 2 each)\")\nprint(\"  • DeepSeek-Math-7B-RL (6 samples: 3 temps × 2 each)\")\nprint(\"  • MAmmoTH-7B-Mistral (3 samples)\")\nprint(\"=\" * 70)\n\nif TEST_MODE:\n    # Testing mode - run on reference.csv to see what models do\n    print(\"\\n[TEST MODE] Running on reference.csv...\")\n    print(\"(Set TEST_MODE = False in cell 0 to submit)\")\n    reference_path = '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv'\n    if not os.path.exists(reference_path):\n        reference_path = '../data/reference.csv'\n\n    if os.path.exists(reference_path):\n        try:\n            ref_df = pd.read_csv(reference_path)\n            correct = 0\n            total = len(ref_df)\n\n            for idx, row in ref_df.iterrows():\n                problem_id = row['id']\n                problem_text = row['problem']\n                expected_answer = int(row['answer'])\n\n                print(f\"\\n{'='*70}\")\n                print(f\"[{idx+1}/{total}] Problem {problem_id}\")\n                print(f\"Expected: {expected_answer}\")\n\n                predicted_answer = solver.solve_problem(problem_id, problem_text)\n                print(f\"Final: {predicted_answer}\")\n\n                if predicted_answer == expected_answer:\n                    print(\"✓ CORRECT\")\n                    correct += 1\n                else:\n                    print(\"✗ INCORRECT\")\n\n            accuracy = (correct / total * 100) if total > 0 else 0\n            print(\"\\n\" + \"=\" * 70)\n            print(f\"SCORE: {correct}/{total} ({accuracy:.1f}%)\")\n            print(\"=\" * 70)\n        except Exception as e:\n            print(f\"Error testing reference.csv: {e}\")\n    \n    # Create dummy submission for Kaggle validation\n    print(\"\\nCreating dummy submission.parquet for Kaggle validation...\")\n    dummy_submission = pl.DataFrame({\n        \"id\": [\"dummy\"],\n        \"answer\": [0]\n    })\n    dummy_submission.write_parquet(\"submission.parquet\")\n    print(\"✓ submission.parquet created\")\n    \nelse:\n    # Submission mode - create inference server and serve\n    print(\"\\n[SUBMISSION MODE] Starting inference server...\")\n    print(\"(Set TEST_MODE = True in cell 0 to test locally)\")\n    \n    def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n        try:\n            question_id = id_.item(0)\n            question_text = problem.item(0)\n            answer = solver.solve_problem(question_id, question_text)\n            return pl.DataFrame({\"id\": [question_id], \"answer\": [answer]})\n        except Exception as e:\n            print(f\"Prediction error: {e}\")\n            return pl.DataFrame({\"id\": [id_.item(0)], \"answer\": [0]})\n    \n    inference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n    inference_server.serve()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}