{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": "# GPT-OSS-20B with TIR and Time Slicing\n# Based on winning AIMO approaches\n\nimport os\nimport sys\n\nos.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport polars as pl\nimport re\nimport subprocess\nimport tempfile\nimport time\nfrom typing import Optional, List, Dict, Tuple\nfrom collections import Counter\nimport kaggle_evaluation.aimo_3_inference_server\n\n# DEBUG: Check what inputs are available\nprint(\"Available inputs in /kaggle/input/:\")\nif os.path.exists('/kaggle/input/'):\n    for item in os.listdir('/kaggle/input/'):\n        print(f\"  - {item}\")\nelse:\n    print(\"  /kaggle/input/ does not exist (running locally)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_python_code(code: str, timeout: int = 10) -> Tuple[Optional[int], str]:\n",
    "    \"\"\"Safely execute Python code and return the result.\"\"\"\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "            f.write(code)\n",
    "            temp_file = f.name\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                ['python3', temp_file],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=timeout\n",
    "            )\n",
    "            \n",
    "            output = result.stdout + result.stderr\n",
    "            \n",
    "            # Extract the last number printed\n",
    "            lines = output.strip().split('\\n')\n",
    "            for line in reversed(lines):\n",
    "                numbers = re.findall(r'\\b(\\d{1,5})\\b', line)\n",
    "                if numbers:\n",
    "                    try:\n",
    "                        answer = int(numbers[-1])\n",
    "                        if 0 <= answer <= 99999:\n",
    "                            return answer, output\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            return None, output\n",
    "        finally:\n",
    "            os.unlink(temp_file)\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def extract_code_from_response(text: str) -> Optional[str]:\n",
    "    \"\"\"Extract Python code blocks from model response.\"\"\"\n",
    "    code_patterns = [\n",
    "        r'```python\\n(.*?)```',\n",
    "        r'```\\n(.*?)```',\n",
    "    ]\n",
    "    \n",
    "    for pattern in code_patterns:\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        if matches:\n",
    "            return matches[-1].strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_answer(text: str) -> Optional[int]:\n",
    "    \"\"\"Extract numerical answer from model output.\"\"\"\n",
    "    patterns = [\n",
    "        r'\\\\boxed\\{(\\d{1,5})\\}',\n",
    "        r'#### (\\d{1,5})',\n",
    "        r'(?:final answer|answer|result|solution)(?:\\s+is)?:?\\s*(\\d{1,5})',\n",
    "        r'=\\s*(\\d{1,5})(?:\\s|$|\\.|,)',\n",
    "        r'(\\d{1,5})(?:\\s+(?:is the|as the) answer)',\n",
    "        r'therefore.*?(\\d{1,5})',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        if matches:\n",
    "            try:\n",
    "                answer = int(matches[-1])\n",
    "                if 0 <= answer <= 99999:\n",
    "                    return answer\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # Try to find any 1-5 digit number near end of text\n",
    "    last_500 = text[-500:] if len(text) > 500 else text\n",
    "    numbers = re.findall(r'\\b(\\d{1,5})\\b', last_500)\n",
    "    if numbers:\n",
    "        try:\n",
    "            answer = int(numbers[-1])\n",
    "            if 0 <= answer <= 99999:\n",
    "                return answer\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "def validate_answer(answer: int) -> bool:\n",
    "    return isinstance(answer, int) and 0 <= answer <= 99999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "class TimeSlicingSolver:\n    \"\"\"GPT-OSS-20B solver with time slicing and TIR.\"\"\"\n    \n    def __init__(self):\n        self.model = None\n        self.tokenizer = None\n        self.model_loaded = False\n        \n        # Time slicing strategies: different time allocations and approaches\n        self.time_slices = [\n            {'name': 'quick_solve', 'max_tokens': 1024, 'temp': 0.3, 'timeout': 5},\n            {'name': 'deep_reasoning', 'max_tokens': 2048, 'temp': 0.5, 'timeout': 10},\n            {'name': 'creative_search', 'max_tokens': 2048, 'temp': 0.8, 'timeout': 10},\n            {'name': 'code_focused', 'max_tokens': 1536, 'temp': 0.4, 'timeout': 8},\n        ]\n    \n    def _load_model(self):\n        \"\"\"Load GPT-OSS-20B model.\"\"\"\n        if self.model_loaded:\n            return\n        \n        try:\n            import torch\n            from transformers import AutoTokenizer, AutoModelForCausalLM\n            \n            # Try Kaggle model input paths for GPT-OSS-20B\n            # Format: /kaggle/input/{model-handle}/transformers/{version}\n            possible_paths = [\n                '/kaggle/input/gpt-oss-20b/transformers/default/1',\n                '/kaggle/input/gpt-oss-20b/transformers',\n                '/kaggle/input/gpt-oss-20b',\n            ]\n            \n            model_path = None\n            for path in possible_paths:\n                print(f\"Checking: {path}\")\n                if os.path.exists(path):\n                    # Check if it has config.json\n                    if os.path.exists(os.path.join(path, 'config.json')):\n                        model_path = path\n                        print(f\"  ✓ Found config.json\")\n                        break\n                    else:\n                        print(f\"  ✗ No config.json\")\n            \n            if not model_path:\n                print(\"\\nGPT-OSS-20B not found in Kaggle inputs\")\n                print(\"Add model via metadata: danielhanchen/gpt-oss-20b/Transformers/default/1\")\n                print(f\"Searched paths: {possible_paths}\")\n                return\n            \n            print(f\"\\nLoading GPT-OSS-20B from {model_path}...\")\n            print(\"Model is natively MXFP4 quantized (21B params, 3.6B active)\")\n            \n            self.tokenizer = AutoTokenizer.from_pretrained(\n                model_path,\n                use_fast=True,\n                trust_remote_code=True\n            )\n            \n            # Load model with native MXFP4 quantization\n            # 20B model should fit in 16GB GPU with bf16\n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.bfloat16,\n                device_map=\"auto\",\n                trust_remote_code=True,\n                low_cpu_mem_usage=True,\n            )\n            \n            self.model.eval()\n            self.model_loaded = True\n            \n            print(f\"✓ GPT-OSS-20B loaded successfully\")\n            \n        except Exception as e:\n            print(f\"Failed to load GPT-OSS-20B: {e}\")\n            import traceback\n            traceback.print_exc()\n    \n    def _get_prompt_for_slice(self, problem: str, slice_config: Dict) -> str:\n        \"\"\"Generate specialized prompt based on time slice strategy.\"\"\"\n        \n        if slice_config['name'] == 'quick_solve':\n            return f\"\"\"Solve this IMO problem quickly and concisely.\n\nProblem: {problem}\n\nGive a direct solution with the final answer as \\\\boxed{{answer}}.\"\"\"\n        \n        elif slice_config['name'] == 'deep_reasoning':\n            return f\"\"\"Solve this International Mathematical Olympiad problem with deep reasoning.\n\nProblem:\n{problem}\n\nThink step-by-step:\n1. Understand what's being asked\n2. Identify the mathematical domain (algebra, number theory, combinatorics, geometry)\n3. Apply relevant theorems and techniques\n4. Work through the solution systematically\n5. Verify your answer makes sense\n\nFinal answer (0-99999) in \\\\boxed{{answer}}.\"\"\"\n        \n        elif slice_config['name'] == 'creative_search':\n            return f\"\"\"Explore multiple approaches to solve this IMO problem.\n\nProblem: {problem}\n\nTry different strategies:\n- Direct computation\n- Pattern recognition\n- Working backwards\n- Special cases or symmetry\n\nShow your work and give final answer as \\\\boxed{{answer}}.\"\"\"\n        \n        else:  # code_focused\n            return f\"\"\"Solve this IMO problem using Python code when helpful.\n\nProblem:\n{problem}\n\nYou can write Python code in ```python blocks to:\n- Perform calculations\n- Check patterns\n- Verify solutions\n- Solve equations\n\nShow reasoning and code, then give final answer as \\\\boxed{{answer}}.\"\"\"\n    \n    def _solve_with_time_slice(self, problem: str, slice_config: Dict) -> Optional[int]:\n        \"\"\"Solve using one time slice strategy.\"\"\"\n        if not self.model_loaded:\n            return None\n        \n        try:\n            import torch\n            \n            prompt = self._get_prompt_for_slice(problem, slice_config)\n            \n            # Use chat template if available\n            if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template:\n                messages = [{\"role\": \"user\", \"content\": prompt}]\n                text = self.tokenizer.apply_chat_template(\n                    messages,\n                    tokenize=False,\n                    add_generation_prompt=True\n                )\n            else:\n                text = prompt\n            \n            inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n            \n            start_time = time.time()\n            \n            with torch.no_grad():\n                outputs = self.model.generate(\n                    **inputs,\n                    max_new_tokens=slice_config['max_tokens'],\n                    temperature=slice_config['temp'],\n                    do_sample=True,\n                    top_p=0.9,\n                    top_k=50,\n                    pad_token_id=self.tokenizer.eos_token_id,\n                )\n            \n            elapsed = time.time() - start_time\n            \n            response = self.tokenizer.decode(\n                outputs[0][inputs['input_ids'].shape[1]:],\n                skip_special_tokens=True\n            )\n            \n            # Try Tool-Integrated Reasoning first\n            code = extract_code_from_response(response)\n            answer = None\n            \n            if code:\n                code_answer, _ = execute_python_code(code, timeout=slice_config['timeout'])\n                if code_answer is not None:\n                    answer = code_answer\n            \n            # Fallback to text extraction\n            if answer is None:\n                answer = extract_answer(response)\n            \n            if answer is not None and validate_answer(answer):\n                print(f\"  [{slice_config['name']}] → {answer} ({elapsed:.1f}s)\")\n                return answer\n            else:\n                print(f\"  [{slice_config['name']}] → no valid answer ({elapsed:.1f}s)\")\n            \n            return None\n            \n        except Exception as e:\n            print(f\"  [{slice_config['name']}] error: {str(e)[:50]}\")\n            return None\n    \n    def solve_with_time_slicing(self, problem: str, num_samples: int = 3) -> Optional[int]:\n        \"\"\"Solve using multiple time slices and self-consistency.\"\"\"\n        self._load_model()\n        \n        if not self.model_loaded:\n            return None\n        \n        all_answers = []\n        \n        print(\"  Running time-sliced inference...\")\n        \n        # Run each time slice strategy\n        for slice_config in self.time_slices:\n            # Multiple samples per strategy for self-consistency\n            for sample_idx in range(num_samples):\n                answer = self._solve_with_time_slice(problem, slice_config)\n                if answer is not None:\n                    all_answers.append(answer)\n        \n        if not all_answers:\n            return None\n        \n        # Voting with consensus\n        vote_counts = Counter(all_answers)\n        best_answer, best_count = vote_counts.most_common(1)[0]\n        \n        print(f\"  Total samples: {len(all_answers)}\")\n        print(f\"  Votes: {dict(vote_counts.most_common(3))}\")\n        print(f\"  Consensus: {best_answer} ({best_count}/{len(all_answers)})\")\n        \n        return best_answer\n    \n    def solve_problem(self, problem_id: str, problem_text: str) -> int:\n        \"\"\"Main solving interface.\"\"\"\n        answer = self.solve_with_time_slicing(problem_text, num_samples=3)\n        \n        if answer is None:\n            answer = 0\n        \n        if not validate_answer(answer):\n            answer = abs(answer) % 100000\n        \n        return answer\n\nsolver = TimeSlicingSolver()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"GPT-OSS-20B with Time Slicing & TIR\")\nprint(\"=\" * 70)\nprint(\"Approach:\")\nprint(\"  ✓ GPT-OSS-20B (21B params, 3.6B active)\")\nprint(\"  ✓ Time Slicing: 4 strategies × 3 samples = 12 attempts\")\nprint(\"  ✓ Tool-Integrated Reasoning (Python execution)\")\nprint(\"  ✓ Self-consistency voting\")\nprint(\"\")\nprint(\"Time Slice Strategies:\")\nprint(\"  1. Quick Solve (1024 tokens, temp=0.3, 5s timeout)\")\nprint(\"  2. Deep Reasoning (2048 tokens, temp=0.5, 10s timeout)\")\nprint(\"  3. Creative Search (2048 tokens, temp=0.8, 10s timeout)\")\nprint(\"  4. Code Focused (1536 tokens, temp=0.4, 8s timeout)\")\nprint(\"=\" * 70)\n\ndef predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n    \"\"\"Prediction function for inference server.\"\"\"\n    try:\n        question_id = id_.item(0)\n        question_text = problem.item(0)\n        answer = solver.solve_problem(question_id, question_text)\n        return pl.DataFrame({\"id\": [question_id], \"answer\": [answer]})\n    except Exception as e:\n        print(f\"Prediction error: {e}\")\n        return pl.DataFrame({\"id\": [id_.item(0)], \"answer\": [0]})\n\ninference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n\n# Auto-detect: competition rerun vs local testing\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    print(\"\\n[COMPETITION MODE] Starting inference server...\")\n    inference_server.serve()\nelse:\n    print(\"\\n[LOCAL MODE] Running local gateway on test.csv...\")\n    inference_server.run_local_gateway(\n        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n    )"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}