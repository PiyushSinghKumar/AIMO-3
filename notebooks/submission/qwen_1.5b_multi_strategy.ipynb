{"cells":[{"cell_type":"code","execution_count":null,"id":"cell-0","metadata":{},"outputs":[],"source":"import os\nimport sys\n\nos.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport polars as pl\nimport re\nfrom typing import Optional\nfrom collections import Counter\nimport sympy as sp\nfrom sympy import symbols, solve\nimport kaggle_evaluation.aimo_3_inference_server"},{"cell_type":"code","execution_count":null,"id":"cell-1","metadata":{},"outputs":[],"source":"def extract_answer(text: str) -> Optional[int]:\n    patterns = [\n        r'\\\\boxed\\{(\\d{1,10})\\}',\n        r'(?:answer|result|solution)(?:\\s+is)?:?\\s*(\\d{1,10})',\n        r'(?:final answer|the answer).*?(\\d{1,10})',\n        r'=\\s*(\\d{1,10})(?:\\s|$|\\.|,)',\n        r'(\\d{1,10})(?:\\s+(?:is the|as the) answer)',\n        r'therefore.*?(\\d{1,10})',\n    ]\n    \n    for pattern in patterns:\n        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n        if matches:\n            try:\n                answer = int(matches[-1])\n                if 0 <= answer <= 99999:\n                    return answer\n            except ValueError:\n                continue\n    return None\n\ndef parse_problem(problem_text: str) -> dict:\n    result = {'text': problem_text, 'modulo': None}\n    modulo_patterns = [\n        r'remainder when.*divided by\\s*(\\d+)',\n        r'modulo\\s*(\\d+)',\n        r'mod\\s*(\\d+)',\n        r'\\(mod\\s*(\\d+)\\)',\n    ]\n    for pattern in modulo_patterns:\n        match = re.search(pattern, problem_text, re.IGNORECASE)\n        if match:\n            result['modulo'] = int(match.group(1))\n            break\n    return result\n\ndef format_answer(answer: int, modulo: Optional[int] = None) -> int:\n    if modulo:\n        answer = answer % modulo\n    return abs(answer) % 100000\n\ndef validate_answer(answer: int) -> bool:\n    return isinstance(answer, int) and 0 <= answer <= 99999"},{"cell_type":"code","execution_count":null,"id":"cell-2","metadata":{},"outputs":[],"source":"class MathTools:\n    def solve_equation(self, equation_str: str, variable: str = 'x') -> list:\n        try:\n            x = symbols(variable)\n            if '=' in equation_str:\n                left, right = equation_str.split('=')\n                eq = sp.sympify(left) - sp.sympify(right)\n            else:\n                eq = sp.sympify(equation_str)\n            return solve(eq, x)\n        except:\n            return []\n    \n    def is_prime(self, n: int) -> bool:\n        return sp.isprime(n)\n    \n    def prime_factors(self, n: int) -> dict:\n        return sp.factorint(n)\n    \n    def divisors(self, n: int) -> list:\n        return sp.divisors(n)\n\nmath_tools = MathTools()"},{"cell_type":"code","execution_count":null,"id":"cell-3","metadata":{},"outputs":[],"source":"class MultiStrategyLLMSolver:\n    \"\"\"Ensemble solver using multiple strategies with same model.\"\"\"\n    \n    def __init__(self, num_strategies=5):\n        self.tools = math_tools\n        self.num_strategies = num_strategies\n        self.llm_tokenizer = None\n        self.llm_model = None\n        \n        # Define multiple solving strategies\n        self.strategies = [\n            {'name': 'step_by_step', 'temp': 0.6, 'top_p': 0.9, 'top_k': 40},\n            {'name': 'creative', 'temp': 0.9, 'top_p': 0.95, 'top_k': 50},\n            {'name': 'focused', 'temp': 0.5, 'top_p': 0.85, 'top_k': 30},\n            {'name': 'exploratory', 'temp': 1.0, 'top_p': 0.95, 'top_k': 60},\n            {'name': 'balanced', 'temp': 0.7, 'top_p': 0.9, 'top_k': 45},\n        ]\n        \n        self.prompt_templates = [\n            \"\"\"Solve this IMO problem step-by-step.\n\nProblem: {problem}\n\nApproach:\n1. Understand what's being asked\n2. Identify key concepts (algebra, number theory, combinatorics, geometry)\n3. Work through systematically\n4. Verify your answer\n5. Give final answer as integer 0-99999 in \\\\boxed{{}}\n\nSolution:\"\"\",\n            \"\"\"You are solving an olympiad math problem.\n\nProblem: {problem}\n\nThink carefully and solve step-by-step. Express your final numerical answer (0-99999) clearly using \\\\boxed{{answer}}.\"\"\",\n            \"\"\"Analyze and solve:\n\n{problem}\n\nBreak this down:\n- What type of problem is this?\n- What techniques apply?\n- What's the solution path?\n\nFinal answer (0-99999) in \\\\boxed{{}}:\"\"\",\n        ]\n    \n    def _init_llm(self):\n        if self.llm_model is not None:\n            return\n        \n        try:\n            import torch\n            from transformers import AutoTokenizer, AutoModelForCausalLM\n            \n            if os.path.exists('/kaggle/input/qwen2-5-math-1-5b-instruct/qwen_math'):\n                model_path = '/kaggle/input/qwen2-5-math-1-5b-instruct/qwen_math'\n            else:\n                model_path = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n            \n            print(f\"Loading LLM: {model_path}\")\n            \n            self.llm_tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n            self.llm_model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                device_map=\"auto\" if torch.cuda.is_available() else None,\n            )\n            \n            if torch.cuda.is_available():\n                self.llm_model.eval()\n            \n            print(f\"LLM ready on {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n        except Exception as e:\n            print(f\"LLM load failed: {e}\")\n    \n    def solve_symbolic(self, problem: str) -> Optional[int]:\n        parsed = parse_problem(problem)\n        if 'solve' in problem.lower() and 'for' in problem.lower():\n            eq_match = re.search(r'solve\\s+(.+?)\\s+for', problem, re.IGNORECASE)\n            if eq_match:\n                equation = eq_match.group(1).strip()\n                try:\n                    solutions = self.tools.solve_equation(equation)\n                    if solutions:\n                        answer = int(solutions[0])\n                        return format_answer(answer, parsed['modulo'])\n                except:\n                    pass\n        return None\n    \n    def solve_with_strategy(self, problem: str, strategy: dict, prompt_idx: int) -> Optional[int]:\n        self._init_llm()\n        if self.llm_model is None:\n            return None\n        \n        try:\n            import torch\n            \n            prompt = self.prompt_templates[prompt_idx % len(self.prompt_templates)].format(problem=problem)\n            \n            messages = [\n                {\"role\": \"system\", \"content\": \"You are an expert mathematician specializing in olympiad problems.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n\n            text = self.llm_tokenizer.apply_chat_template(\n                messages,\n                tokenize=False,\n                add_generation_prompt=True\n            )\n\n            inputs = self.llm_tokenizer([text], return_tensors=\"pt\")\n            if hasattr(self.llm_model, 'device'):\n                inputs = inputs.to(self.llm_model.device)\n\n            with torch.no_grad():\n                outputs = self.llm_model.generate(\n                    **inputs,\n                    max_new_tokens=2048,\n                    temperature=strategy['temp'],\n                    do_sample=True,\n                    top_p=strategy['top_p'],\n                    top_k=strategy['top_k'],\n                    pad_token_id=self.llm_tokenizer.eos_token_id,\n                )\n\n            response = self.llm_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n            return extract_answer(response)\n        except Exception as e:\n            print(f\"Strategy {strategy['name']} error: {e}\")\n            return None\n    \n    def solve_ensemble(self, problem: str) -> Optional[int]:\n        \"\"\"Run multiple strategies and vote.\"\"\"\n        answers = []\n        \n        for i, strategy in enumerate(self.strategies[:self.num_strategies]):\n            answer = self.solve_with_strategy(problem, strategy, i)\n            if answer is not None:\n                answers.append(answer)\n        \n        if not answers:\n            return None\n        \n        # Weighted voting: give more weight to consensus\n        vote_counts = Counter(answers)\n        best_answer, count = vote_counts.most_common(1)[0]\n        \n        if len(answers) > 1:\n            print(f\"Strategies: {answers} → {best_answer} ({count}/{len(answers)} votes)\")\n        \n        return best_answer\n    \n    def solve_problem(self, problem_id: str, problem_text: str) -> int:\n        # Try symbolic first\n        answer = self.solve_symbolic(problem_text)\n        \n        # Fall back to ensemble\n        if answer is None:\n            answer = self.solve_ensemble(problem_text)\n        \n        # Default to 0\n        if answer is None:\n            answer = 0\n        \n        if not validate_answer(answer):\n            answer = abs(answer) % 100000\n        \n        return answer\n\nsolver = MultiStrategyLLMSolver(num_strategies=5)"},{"cell_type":"code","execution_count":null,"id":"cell-4","metadata":{},"outputs":[],"source":"def predict(id_: pl.Series, problem: pl.Series) -> pl.DataFrame:\n    try:\n        question_id = id_.item(0)\n        question_text = problem.item(0)\n        answer = solver.solve_problem(question_id, question_text)\n        return pl.DataFrame({\"id\": [question_id], \"answer\": [answer]})\n    except Exception as e:\n        print(f\"Prediction error: {e}\")\n        return pl.DataFrame({\"id\": [id_.item(0)], \"answer\": [0]})\n\ninference_server = kaggle_evaluation.aimo_3_inference_server.AIMO3InferenceServer(predict)\n\n# Test on reference.csv (real IMO problems) for evaluation\nprint(\"=\" * 60)\nprint(\"TESTING ON REFERENCE.CSV (IMO-LEVEL PROBLEMS)\")\nprint(\"=\" * 60)\n\nreference_path = '/kaggle/input/ai-mathematical-olympiad-progress-prize-3/reference.csv'\nif not os.path.exists(reference_path):\n    reference_path = '../data/reference.csv'\n\nif os.path.exists(reference_path):\n    try:\n        ref_df = pd.read_csv(reference_path)\n        correct = 0\n        total = len(ref_df)\n\n        for idx, row in ref_df.iterrows():\n            problem_id = row['id']\n            problem_text = row['problem']\n            expected_answer = int(row['answer'])\n\n            print(f\"\\n[{idx+1}/{total}] Problem {problem_id}\")\n            print(f\"Expected: {expected_answer}\")\n\n            predicted_answer = solver.solve_problem(problem_id, problem_text)\n            print(f\"Predicted: {predicted_answer}\")\n\n            if predicted_answer == expected_answer:\n                print(\"✓ CORRECT\")\n                correct += 1\n            else:\n                print(\"✗ INCORRECT\")\n\n        accuracy = (correct / total * 100) if total > 0 else 0\n        print(\"\\n\" + \"=\" * 60)\n        print(f\"REFERENCE.CSV SCORE: {correct}/{total} ({accuracy:.1f}%)\")\n        print(\"=\" * 60 + \"\\n\")\n    except Exception as e:\n        print(f\"Error testing reference.csv: {e}\")\nelse:\n    print(\"reference.csv not found, skipping evaluation\\n\")\n\n# Run inference server or local gateway\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    print(\"Running local gateway on test.csv...\")\n    inference_server.run_local_gateway(\n        ('/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv',)\n    )"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.0"}},"nbformat":4,"nbformat_minor":5}